# ETLpipeline_design_from_kaggle_api_snowflake_andorcgestrated_with_mageai
####  The City Bike Project is a data engineering project aimed at analyzing and visualizing bike-sharing data from a city's bike-sharing system. The project involves extracting data from a public dataset, performing data transformations, and providing insights into bike usage patterns such as trip duration, popular locations, user demographics, and trip frequency. The goal is to deliver valuable information that can help in optimizing city bike services and improving urban mobility.
The dataset used in this project typically contains information on individual bike trips, including the start and end times, locations, trip duration, and other related data points. By processing and analyzing this data, we can answer key questions like:

1. What is the average duration of a bike ride?
2. How does usage vary by time of day or day of the week?
3. Which bike stations are the most popular?
4.  What are the demographic patterns of bike users?
#### Dataset Overview
#### The dataset used for this analysis includes the following key attributes:

1. Trip ID: Unique identifier for each trip.
2. Start Time: Timestamp of when the bike trip started.
3. End Time: Timestamp of when the bike trip ended.
4. Duration: Duration of the trip in seconds.
5. Start Station: The bike station where the trip began.
6. End Station: The bike station where the trip ended.
7. User Type: Whether the user is a "Subscriber" or a "Customer."

#### Tech stack for the project 
1. python for scripting
2. mageai for work flow orchestration
3. docker for conatainerization
4. pgadmin for postgres quuery
5. postgres database as datalake
6. snowflake data warehouse
7. snowsql for quarying

### Data Collection and ETL Process:
The data for this project was sourced from Kaggle through their public API, utilizing an API key for secure access. The API key was downloaded and stored in the directory of the Python script to facilitate the data extraction process.
The extraction process was implemented using Python, where data was retrieved programmatically from Kaggle's API. Following this, the dataset underwent a thorough cleaning process using Pythonâ€™s data manipulation libraries. This step involved handling missing values, removing duplicates, and standardizing the dataset to ensure consistency and accuracy.
Once the data was cleaned, it was loaded into a PostgreSQL database, thus completing the ETL (Extract, Transform, Load) pipeline. The data was transformed within Python as part of the process, ensuring that it was structured appropriately for efficient querying and analysis within the database.
![load_data_postgres_database](https://github.com/rukevweubio/ETLpipeline_design_from_kaggle_api_snowflake_andorchestrated_with_mageai/blob/main/Screenshot%20(198).png)


   
